{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec665ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 'artifacts' package importable from this notebook\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"app.py\").exists() or (p / \"requirements.txt\").exists() or (p / \".git\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "ROOT = find_repo_root(Path.cwd())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"Repo root:\", ROOT)\n",
    "print(\"Artifacts directory exists:\", (ROOT / \"artifacts\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26145591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve artifact path similar to the Streamlit app\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_artifact_path(preferred: str) -> str:\n",
    "    p = Path(preferred)\n",
    "    # If absolute and exists, return\n",
    "    try:\n",
    "        if p.is_absolute() and p.exists():\n",
    "            return str(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "    name = p.name\n",
    "    candidates = [\n",
    "        ROOT / preferred,  # repo-root relative (e.g., models/NAME.joblib)\n",
    "        ROOT / \"artifacts/notebooks/models\" / name,\n",
    "        ROOT / \"artifacts/notebooks\" / name,\n",
    "        ROOT / \"models\" / name,\n",
    "        ROOT / \"artifacts/models\" / name,\n",
    "        p,  # original as-is\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            if Path(c).exists():\n",
    "                return str(c)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return str(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bda3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to normalize yfinance DataFrame for ensure_schema()\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_yf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Flatten MultiIndex columns to first level (e.g., ('Open','') -> 'Open')\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [c[0] if isinstance(c, tuple) else c for c in df.columns]\n",
    "    # Standardize date column naming to 'ds' expected by ensure_schema\n",
    "    if \"Date\" in df.columns:\n",
    "        df = df.rename(columns={\"Date\": \"ds\"})\n",
    "    elif \"Datetime\" in df.columns:\n",
    "        df = df.rename(columns={\"Datetime\": \"ds\"})\n",
    "    # If still missing, move datetime index to column\n",
    "    if \"ds\" not in df.columns and isinstance(df.index, pd.DatetimeIndex):\n",
    "        df = df.reset_index().rename(columns={\"index\": \"ds\"})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8cb99",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0833bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from darts import TimeSeries\n",
    "from darts.models import NHiTSModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸš€ Starting N-HiTS Model...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check GPU availability\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ðŸ”§ Using accelerator: {accelerator}\")\n",
    "pl_trainer_kwargs = {\n",
    "    \"accelerator\": accelerator,\n",
    "    \"devices\": 1,\n",
    "} if accelerator == \"gpu\" else {}\n",
    "\n",
    "emiten = 'ELSA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global seeds for reproducibility\n",
    "\n",
    "import random\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Use deterministic algorithms where possible (CPU-safe)\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba89779",
   "metadata": {},
   "source": [
    "### Fungsi untuk Rekayasa Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2835bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Menciptakan fitur-fitur teknikal (sesuai dengan hybrid model)\"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    # Price Change (Target untuk N-HiTS)\n",
    "    data['Price_Change'] = data['Close'].pct_change()\n",
    "    \n",
    "    # Previous Close\n",
    "    data['Prev Close'] = data['Close'].shift(1)\n",
    "\n",
    "    # Moving Averages\n",
    "    data['MA20'] = data['Close'].rolling(window=20).mean()\n",
    "    data['MA50'] = data['Close'].rolling(window=50).mean()\n",
    "\n",
    "    # RSI\n",
    "    delta = data['Close'].diff(1)\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    gain_avg = gain.rolling(window=14).mean()\n",
    "    loss_avg = loss.rolling(window=14).mean()\n",
    "    rs = gain_avg / (loss_avg + 1e-8)\n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # MACD\n",
    "    data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "    data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD'] = data['EMA12'] - data['EMA26']\n",
    "    data['Signal Line'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # Bollinger Bands\n",
    "    data['BB_Upper'] = data['Close'].rolling(window=20).mean() + 2*data['Close'].rolling(window=20).std()\n",
    "    data['BB_Lower'] = data['Close'].rolling(window=20).mean() - 2*data['Close'].rolling(window=20).std()\n",
    "\n",
    "    # ATR\n",
    "    high_low = data['High'] - data['Low']\n",
    "    high_close = abs(data['High'] - data['Close'].shift(1))\n",
    "    low_close = abs(data['Low'] - data['Close'].shift(1))\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    data['ATR'] = true_range.max(axis=1).rolling(window=14).mean()\n",
    "\n",
    "    # Stochastic\n",
    "    data['Stochastic_K'] = 100 * (data['Close'] - data['Low'].rolling(window=14).min()) / \\\n",
    "                           (data['High'].rolling(window=14).max() - data['Low'].rolling(window=14).min())\n",
    "    data['Stochastic_D'] = data['Stochastic_K'].rolling(window=3).mean()\n",
    "\n",
    "    # CCI\n",
    "    data['CCI'] = (data['Close'] - data['Close'].rolling(window=20).mean()) / \\\n",
    "                  (0.015 * data['Close'].rolling(window=20).std())\n",
    "\n",
    "    # OBV\n",
    "    data['OBV'] = np.where(data['Close'] > data['Close'].shift(1), data['Volume'],\n",
    "                           np.where(data['Close'] < data['Close'].shift(1), -data['Volume'], 0))\n",
    "    data['OBV'] = data['OBV'].cumsum()\n",
    "\n",
    "    # Lag features\n",
    "    for i in range(1, 6):\n",
    "        data[f'Lag{i}'] = data['Close'].shift(i)\n",
    "\n",
    "    data = data.dropna().reset_index()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e53462",
   "metadata": {},
   "source": [
    "### LANGKAH 1: Download Data & Rekayasa Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL: Download data & create features ===\n",
    "# Set ticker here (single-stock mode) - SAME AS PROPHET\n",
    "ticker = emiten + '.JK'\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ“Š STOCK: {ticker}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nðŸ“¥ Downloading {ticker} data...\")\n",
    "data_raw = yf.download(ticker, start='2020-01-01', end='2025-01-01', progress=False)\n",
    "\n",
    "if isinstance(data_raw.columns, pd.MultiIndex):\n",
    "    data_raw.columns = data_raw.columns.get_level_values(0)\n",
    "\n",
    "# Ensure daily frequency and forward-fill missing\n",
    "data_raw = data_raw.asfreq('D').ffill()\n",
    "print(f\"âœ… Data downloaded: {data_raw.shape}\")\n",
    "\n",
    "print(f\"ðŸ”§ Creating technical indicators...\")\n",
    "data_featured = create_features(data_raw)\n",
    "print(f\"âœ… Features created: {data_featured.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3789726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL: Data preparation (features -> TimeSeries, split)\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Prepare dataframe for model\n",
    "df_prophet = data_featured.rename(columns={'Date': 'ds', 'Close': 'y'})\n",
    "\n",
    "selected_features = [\n",
    "    'Prev Close', 'MA20', 'MA50', 'MACD', 'Signal Line',\n",
    "    'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'RSI',\n",
    "    'BB_Upper', 'BB_Lower', 'ATR', 'Stochastic_K', 'Stochastic_D', 'CCI', 'OBV', 'Volume'\n",
    "]\n",
    "\n",
    "columns_for_model = ['ds', 'y'] + selected_features\n",
    "\n",
    "df_model_ready = df_prophet[columns_for_model].copy()\n",
    "df_model_ready['ds'] = pd.to_datetime(df_model_ready['ds'])\n",
    "# Ensure daily frequency and forward-fill to avoid NaNs introduced by asfreq\n",
    "df_model_ready = df_model_ready.set_index('ds').asfreq('D').ffill().reset_index()\n",
    "# Drop any remaining NaNs in target or features\n",
    "df_model_ready = df_model_ready.dropna(subset=['y'] + selected_features).reset_index(drop=True)\n",
    "\n",
    "# Split\n",
    "train_size_full = int(len(df_model_ready) * 0.8)\n",
    "train_df_pd = df_model_ready.iloc[:train_size_full]\n",
    "test_df_pd = df_model_ready.iloc[train_size_full:]\n",
    "\n",
    "print(f\"ðŸ“Š Split: {len(train_df_pd)} train, {len(test_df_pd)} test\")\n",
    "# Optional sanity checks\n",
    "if train_df_pd.isna().any().any() or test_df_pd.isna().any().any():\n",
    "    print(\"âš ï¸ Warning: NaNs detected after split; consider inspecting df_model_ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad61669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL: Scaling and Training ===\n",
    "# Convert to Darts TimeSeries and scale\n",
    "train_y = TimeSeries.from_dataframe(train_df_pd, 'ds', 'y', freq='D')\n",
    "train_cov = TimeSeries.from_dataframe(train_df_pd, 'ds', selected_features, freq='D')\n",
    "\n",
    "mm_scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "mm_scaler_cov = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = Scaler(scaler=mm_scaler_y)\n",
    "scaler_cov = Scaler(scaler=mm_scaler_cov)\n",
    "\n",
    "train_y_s = scaler_y.fit_transform(train_y)\n",
    "train_cov_s = scaler_cov.fit_transform(train_cov)\n",
    "\n",
    "# Training configuration\n",
    "print(\"âš¡ Training N-HiTS...\")\n",
    "early_stopping = EarlyStopping(monitor=\"train_loss\", patience=10, mode=\"min\")\n",
    "trainer_kwargs = pl_trainer_kwargs.copy()\n",
    "trainer_kwargs[\"callbacks\"] = [early_stopping]\n",
    "trainer_kwargs[\"enable_progress_bar\"] = True\n",
    "trainer_kwargs[\"limit_train_batches\"] = 0.8\n",
    "trainer_kwargs[\"logger\"] = False\n",
    "\n",
    "model = NHiTSModel(\n",
    "    input_chunk_length=60,\n",
    "    output_chunk_length=5,  # match app/test horizon (e.g., 5)\n",
    "    num_stacks=3,\n",
    "    num_blocks=2,\n",
    "    num_layers=3,\n",
    "    layer_widths=192,\n",
    "    n_epochs=50,\n",
    "    dropout=0.20,\n",
    "    batch_size=64,\n",
    "    random_state=42,\n",
    "    activation='ReLU',\n",
    "    MaxPool1d=True,\n",
    "    pl_trainer_kwargs=trainer_kwargs,\n",
    "    show_warnings=False,\n",
    "    save_checkpoints=False,\n",
    "    force_reset=True\n",
    ")\n",
    "\n",
    "model.fit(train_y_s, past_covariates=train_cov_s, verbose=True, max_samples_per_ts=600)\n",
    "print(\"âœ… Training completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL: Prediction ===\n",
    "# Prepare covariates for the full period and predict\n",
    "full_cov_pd = df_model_ready\n",
    "full_cov_ts = TimeSeries.from_dataframe(full_cov_pd, 'ds', selected_features, freq='D')\n",
    "full_cov_ts_s = scaler_cov.transform(full_cov_ts)\n",
    "\n",
    "forecast_s = model.predict(n=len(test_df_pd), series=train_y_s, past_covariates=full_cov_ts_s)\n",
    "forecast_unscaled = scaler_y.inverse_transform(forecast_s)\n",
    "reconstructed_prices = forecast_unscaled.values().flatten()\n",
    "\n",
    "original_train_size = len(train_df_pd) + (len(data_featured) - len(df_model_ready))\n",
    "y_test_actual = data_featured['Close'].iloc[original_train_size:].values\n",
    "min_len = min(len(y_test_actual), len(reconstructed_prices))\n",
    "y_test_actual = y_test_actual[:min_len]\n",
    "reconstructed_prices = reconstructed_prices[:min_len]\n",
    "\n",
    "# Simpan hasil raw\n",
    "reconstructed_prices_raw = reconstructed_prices.copy()\n",
    "\n",
    "print(\"âœ… Prediction completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL: Evaluation (base metrics) ===\n",
    "# Align actual vs predicted using the test slice from df_model_ready\n",
    "# and drop NaNs / non-finite values before computing metrics.\n",
    "\n",
    "# Ground truth from the prepared model dataframe (matches forecast horizon)\n",
    "actual_full = df_model_ready['y'].iloc[train_size_full:].to_numpy()\n",
    "pred_full = reconstructed_prices\n",
    "\n",
    "# Align lengths\n",
    "min_len_eval = min(len(actual_full), len(pred_full))\n",
    "actual = actual_full[:min_len_eval]\n",
    "pred = pred_full[:min_len_eval]\n",
    "\n",
    "# Drop NaNs and non-finite values\n",
    "valid_mask = np.isfinite(actual) & np.isfinite(pred)\n",
    "actual = actual[valid_mask]\n",
    "pred = pred[valid_mask]\n",
    "\n",
    "# Persist back for later cells\n",
    "y_test_actual = actual\n",
    "reconstructed_prices = pred\n",
    "\n",
    "if y_test_actual.size == 0:\n",
    "    raise ValueError(\"No valid samples for evaluation after removing NaNs.\")\n",
    "\n",
    "mae = mean_absolute_error(y_test_actual, reconstructed_prices)\n",
    "mse = mean_squared_error(y_test_actual, reconstructed_prices)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = r2_score(y_test_actual, reconstructed_prices)\n",
    "# MAPE: avoid divide-by-zero using epsilon\n",
    "eps = 1e-8\n",
    "mape = np.mean(np.abs((y_test_actual - reconstructed_prices) / np.where(y_test_actual == 0, eps, y_test_actual))) * 100\n",
    "\n",
    "# Directional Accuracy (guard when too short)\n",
    "T = len(y_test_actual) - 1\n",
    "if T > 0:\n",
    "    actual_diff = np.diff(y_test_actual)\n",
    "    pred_diff = np.diff(reconstructed_prices)\n",
    "    ai = (np.sign(actual_diff) == np.sign(pred_diff)).astype(int)\n",
    "    da = (1/T) * np.sum(ai) * 100\n",
    "else:\n",
    "    da = float('nan')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ“Š BASE MODEL RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"RMSE  : {rmse:.4f}\")\n",
    "print(f\"MAE   : {mae:.4f}\")\n",
    "print(f\"RÂ²    : {r2:.4f}\")\n",
    "print(f\"MAPE  : {mape:.2f}%\")\n",
    "print(f\"DA    : {da:.2f}%\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28fe39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL: Final Evaluation (NO POST-PROCESSING to be fair) ===\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ“Š FINAL N-HiTS RESULTS (Apple-to-Apple with Prophet)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Ticker    : {ticker}\")\n",
    "print(f\"RMSE      : {rmse:.4f}\")\n",
    "print(f\"MAE       : {mae:.4f}\")\n",
    "print(f\"RÂ²        : {r2:.4f}\")\n",
    "print(f\"MAPE      : {mape:.2f}%\")\n",
    "print(f\"DA        : {da:.2f}%\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Additional error statistics\n",
    "nhits_errors = np.abs(y_test_actual - reconstructed_prices)\n",
    "print(f\"\\n   - Average Error: {np.mean(nhits_errors):.2f} IDR\")\n",
    "print(f\"   - Max Error: {np.max(nhits_errors):.2f} IDR\")\n",
    "print(f\"   - Min Error: {np.min(nhits_errors):.2f} IDR\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ N-HiTS Model Analysis Completed!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb70218",
   "metadata": {},
   "source": [
    "### LANGKAH 7: Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1284901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Creating visualizations...\")\n",
    "\n",
    "# Build test dates aligned to cleaned evaluation arrays (after masking)\n",
    "# Using df_model_ready ensures consistency with evaluation\n",
    "all_test_dates = df_model_ready['ds'].iloc[train_size_full:]\n",
    "# Use the last len(y_test_actual) dates to align with potentially shorter arrays\n",
    "if len(all_test_dates) >= len(y_test_actual):\n",
    "    test_dates = all_test_dates.iloc[:len(y_test_actual)].to_numpy()\n",
    "else:\n",
    "    # Fallback: truncate to available\n",
    "    test_dates = all_test_dates.to_numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Time Series\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(test_dates, y_test_actual, label='Actual Price', color='black', linewidth=2.5, alpha=0.8)\n",
    "ax1.plot(test_dates, reconstructed_prices, label='N-HiTS Prediction', color='red',\n",
    "         linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax1.set_title('ðŸ“ˆ Actual vs N-HiTS Prediction', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Date', fontsize=11)\n",
    "ax1.set_ylabel('Price (IDR)', fontsize=11)\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter Plot\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.scatter(y_test_actual, reconstructed_prices, alpha=0.6, color='red', s=30)\n",
    "ax2.plot([y_test_actual.min(), y_test_actual.max()], [y_test_actual.min(), y_test_actual.max()],\n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('Actual Price', fontsize=10)\n",
    "ax2.set_ylabel('N-HiTS Prediction', fontsize=10)\n",
    "ax2.set_title(f'N-HiTS: RÂ² = {r2:.4f}', fontsize=11)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "nhits_residuals = y_test_actual - reconstructed_prices\n",
    "ax3.hist(nhits_residuals, bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "ax3.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_xlabel('Prediction Error', fontsize=10)\n",
    "ax3.set_ylabel('Frequency', fontsize=10)\n",
    "ax3.set_title('Error Distribution', fontsize=11)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Metrics Bar Chart\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "metrics = ['RMSE', 'MAE', 'MAPE']\n",
    "metric_values = [rmse, mae, mape]\n",
    "bars = ax4.bar(metrics, metric_values, color=['#e74c3c', '#3498db', '#2ecc71'], alpha=0.7, edgecolor='black')\n",
    "ax4.set_ylabel('Error Value', fontsize=10)\n",
    "ax4.set_title('Error Metrics', fontsize=11, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "for bar, value in zip(bars, metric_values):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{value:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 5: Directional Accuracy\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "da_data = [da if np.isfinite(da) else 0, (100-da) if np.isfinite(da) else 100]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "labels = ['Correct Direction', 'Wrong Direction']\n",
    "wedges, texts, autotexts = ax5.pie(da_data, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "                                    startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax5.set_title(f'Directional Accuracy: {da:.2f}%', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'ðŸ“Š N-HiTS Model - Comprehensive Analysis ({ticker})',\n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('nhits_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ… Visualization saved as 'nhits_results.png'\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export N-HiTS artifact to repository 'models' with scalers + features\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from darts.models import NHiTSModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "# Prefer the trained variables from above\n",
    "nhits_model = globals().get('model', None)  # NHiTSModel instance\n",
    "scaler_y = globals().get('scaler_y', None)  # Darts Scaler for target\n",
    "scaler_cov = globals().get('scaler_cov', None)  # Darts Scaler for covariates\n",
    "feature_cols = globals().get('selected_features', None)\n",
    "ticker_val = globals().get('ticker', None)\n",
    "\n",
    "if nhits_model is None or not isinstance(nhits_model, NHiTSModel):\n",
    "    raise RuntimeError(\"No NHiTSModel instance found (variable `model`). Run training cells first.\")\n",
    "if scaler_y is None or not isinstance(scaler_y, Scaler):\n",
    "    raise RuntimeError(\"No fitted Darts Scaler found for target (`scaler_y`). Run training cells first.\")\n",
    "if scaler_cov is None or not isinstance(scaler_cov, Scaler):\n",
    "    raise RuntimeError(\"No fitted Darts Scaler found for covariates (`scaler_cov`). Run training cells first.\")\n",
    "if feature_cols is None or len(feature_cols) == 0:\n",
    "    # Fallback: infer features from a DataFrame that contains ds/y and indicators\n",
    "    candidate_dfs = [v for v in globals().values() if isinstance(v, pd.DataFrame)]\n",
    "    known = {'Prev Close','MA20','MA50','MACD','Signal Line','Lag1','Lag2','Lag3','Lag4','Lag5','RSI','BB_Upper','BB_Lower','ATR','Stochastic_K','Stochastic_D','CCI','OBV','High','Low','Volume'}\n",
    "    for d in candidate_dfs:\n",
    "        cols = list(d.columns)\n",
    "        if 'ds' in cols and ('y' in cols or 'Close' in cols):\n",
    "            feats = [c for c in cols if c not in ['ds','y','Close']]\n",
    "            if len(set(feats) & known) >= 5:\n",
    "                feature_cols = feats\n",
    "                break\n",
    "if feature_cols is None or len(feature_cols) == 0:\n",
    "    raise RuntimeError('Could not determine feature_columns; ensure `selected_features` is defined.')\n",
    "\n",
    "# Derive consistent base name from ticker (preferred) to avoid mismatches\n",
    "if ticker_val and isinstance(ticker_val, str):\n",
    "    base_name = ticker_val.split('.')[0].upper()\n",
    "else:\n",
    "    # Fallback to any earlier symbol like `emiten`, else generic name\n",
    "    base_name = str(globals().get('emiten', 'MODEL')).split('.')[0].upper()\n",
    "\n",
    "# Optional sanity print\n",
    "print(f\"ðŸ—‚ Export base name: {base_name}\")\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / 'requirements.txt').exists() or (p / 'app.py').exists() or (p / '.git').exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "repo_root = find_repo_root(Path.cwd())\n",
    "export_dir = (repo_root / 'models')\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model via Darts native serialization for reliable predict\n",
    "nhits_path = export_dir / f\"{base_name}_nhits.darts\"\n",
    "nhits_model.save(str(nhits_path))  # Darts expects a string path\n",
    "\n",
    "metrics = {\n",
    "    'rmse': float(globals().get('rmse')) if 'rmse' in globals() else None,\n",
    "    'mae': float(globals().get('mae')) if 'mae' in globals() else None,\n",
    "    'mape': float(globals().get('mape')) if 'mape' in globals() else None,\n",
    "    'r2': float(globals().get('r2')) if 'r2' in globals() else None,\n",
    "    'directional_accuracy': float(globals().get('da')) if 'da' in globals() else None,\n",
    "}\n",
    "\n",
    "artifact = {\n",
    "    'model_type': 'nhits',\n",
    "    'nhits_path': str(nhits_path.resolve()),\n",
    "    'scaler_y': scaler_y,\n",
    "    'scaler_cov': scaler_cov,\n",
    "    'feature_columns': feature_cols,\n",
    "    'ticker': ticker_val,\n",
    "    'metrics': metrics,\n",
    "}\n",
    "artifact_path = export_dir / f\"{base_name}_nhits.joblib\"\n",
    "joblib.dump(artifact, str(artifact_path))\n",
    "print('âœ… Saved NHITS artifact:', artifact_path.resolve())\n",
    "print('âœ… Saved NHITS model:', nhits_path.resolve())\n",
    "# Quick confirmation\n",
    "try:\n",
    "    print('ðŸ”Ž Model output_chunk_length:', getattr(nhits_model, 'output_chunk_length', 'N/A'))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
